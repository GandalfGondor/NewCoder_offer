第一题、有10个G的数据，如果两条数据一样，则表示该两条数据重复了，现在给你512的内存，把这10G中重复次
数最高的10条数据取出来。
思路1：
【1】先排序， 10G数据分成40份，每份256M，排序，合并相同数据并加上计数器，写到临时文件chunk01~chunk20。
【2】对每一chunk, 读入内存，对每一条数据，再依次读入其后续个chunk, 合并相同数据的计数，后写入一个文
件count。为了避免重复计数，在计数累加后需要将原来chunk的计数清零并回写文件。以chunk01为例。假设chunk01
中有数据A-8(数据A, 8次)，chunk02中有A-2，那么合并chunk02后chunk01的内存中为A-10， chunk02中为A-0，这时
把chunk02写回文件，然后读入chunk03继续处理处理。最后把chunk01中计数不为0的数据(chunk01里不会有计数为0
的，但是后面的chunk会有)写入文件count.
【3】对count文件进行按重复次数排序。（分组，排序，然后每组选前10，再排序）

思路2：
一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。
这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并
排序就可以了。

第二题、对5亿数据进行排序
思路：
1、首先分割大文件为小文件。
2、对小文件进行排序。
3、进行合并。
重点就是如何进行合并：
方式一：我们可以把每个文件中的数据看成是一个队列，我们总是从队列的首部开始进行出队
(因为队列的头部总是最小的数)。这样，我们就把问题转化成从N个小文件中依次比较，得到最小的结果并记入
文件(当然，我们不可以生成一个数就写一次文件，这样太低效了，我们可以使用一个变量缓存这此"最小值"，在
累计到一定数量之后再一次性写入。再清空变量，循环反复，直到文件全部写入完毕)。

方式二：首先遍历1000个文件，每个文件里面取第一个数字，组成 (数字, 文件号) 这样的组合加入到堆里
（假设是从小到大排序，用小顶堆），遍历完后堆里有1000个 (数字，文件号) 这样的元素然后不断从堆顶拿元素出
来，每拿出一个元素，把它的文件号读取出来，然后去对应的文件里，加一个元素进入堆，直到那个文件被读取完。
拿出来的元素当然追加到最终结果的文件里。按照上面的操作，直到堆被取空了，此时最终结果文件里的全部数字
就是有序的了。

方式三：如果数据没有重复值，则用bitmap排序方法。


第三题、给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
策略：分治方法
1、10亿为1G，所以字符串总内存为320G。分成1000份，比较合理。
2、将a和b两个文件按照相同的方式进行分割，分割到1000个小文件之中，利用hash(url)%1000，这样只有对应的
小文件可能会出现相同的文件。
3、求每一对文件中相同的URL，通过hash_set方法找相同的字符串。

策略：有一定错误概率的方式，利用bloom filter方式。

第四题、在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。
策略：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，
共需内存1G内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，
10保持不变。所描完，查看bitmap，把对应位是01的整数输出即可。



